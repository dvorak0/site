+++
title = "重新认识Chris Lattner"
date = 2025-10-27T16:12:00+08:00
tags = ["PUBLIC"]
draft = false
+++

Chris是谁？

-   在Apple发明LLVM/Clang/Swift。
-   在Tesla另外自动驾驶芯片。
-   在Google发起MLIR
-   RISC-V商业化公司SiFive的VP
-   Modulr的创始人，发明Mojo

一系列光环的大佬。

记录这个文章的起点是，发现Chris在和geohot（[重新认识geohot]({{< relref "20230510165215-重新认识geohot.md" >}})中的geohot）就什么是AI compiler的终极形态的讨论：

{{< figure src="/ox-hugo/2025-10-27_16-16-01_screenshot.png" >}}

可以说是正统派和野路子的正面交锋/交流。

Chris写了一个系列文章：[Democratizing AI Compute](https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai)，讲述了自己为什么要创业做Mojo。

读完之后，感觉终于解决了我一个长久的困惑，TVM/XLA/Triton/cuTile...似乎都在解决一个问题，AI编译器，他们究竟是什么关系。

所幸大佬站在浪潮之巅，亲身经历这一切，并慷慨地写了一个像是自传一样的总结，让我可以管中窥豹。

记录一下。

<!--more-->


## DeepSeek对AI的影响 {#deepseek对ai的影响}

DeepSeek通过透明的承认自己在压榨GPU上的热爱和成就，戳破了一层窗户纸。

AI模型的工程化比学术化相比，更加是这个时代的中心，而GPU的利用水平拉开了不同玩家的差距。

Chris过去25年最多的时间投入在了LLVM上，这是一个编译器的编译器，为C++/Rust/Swift/OpenCL的语言奠定了编译器基础。

其中OpenCL则是最早的大家对于通用AI编译器抽象的愿景的化身。

在Google，Chris帮助Google做出了TPU，可以说是帮Attention/Bert等创新铺平了计算道路。

但是当别人问他：为什么CUDA这种东西依旧存在。Chris自己也觉得需要展开说说：

-   为什么CUDA如此成功
-   为什么OpenCL并没有解决这个问题，而后继者Triton，OneAPI也没有解决这个问题。


## 到底什么是CUDA {#到底什么是cuda}

是语法（CUDA C++），还是库（cuDNN）或者是解决方案（Triton Serving/TensorRT/TensorRT-LLM）。

2001年之前，GPU只能运行固定的render pipeline（transform/lightning/raster），直到shader开始支持可编程的图形效果。

人们发现，这里存在通用计算的可能。

2006年CUDA发布，用一种C++语法扩展的方案支持了并行计算。

然后很快地，在这种语言之上的库蓬勃发展，线性代数cuBLAS，傅里叶变换cuFFT，以及对后面影响最大的神经网络cuDNN。直接支撑起了TensorFlow和PyTorch。

然后又有：

-   Triton Serving：模型推理服务
-   TensorRT：端测的模型编译部署
-   TensorRT-LLM：Nvidia的类似vLLM的LLM推理服务


## CUDA怎么成功的 {#cuda怎么成功的}

只能马后炮的总结一下了：跨代兼容，坚定的投入软件，和PyTorch/TensorFlow并肩作战，AI迎来了GenAI这样真正的突破。


## CUDA足够好吗 {#cuda足够好吗}

垄断是每个人（除了nvidia）都担心的。

CUDA不具有100%的硬件控制力，所以FlashAttention不得不使用PTX来写。相比CUDA来说，PTX就更加是一个黑匣子了。

所以[Jim Keller](https://en.wikipedia.org/wiki/Jim_Keller_(engineer))有一个有名的论述：[CUDA是一片沼泽，而不是护城河](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too)。


## OpenCL怎么样 {#opencl怎么样}

OpenCL肩负了大家的愿景，一个硬件无关的AI计算标准，但是失败了。特别是发起者Apple自己都放弃了OpenCL，推出了自己的Metal。

Chris从中吸取了教训：

-   不能只提供规范，没有参考实现。
-   强有力的领导，不能随意分叉，碎片化。
-   性能要顶级。目前OpenCL竟然都无法利用TensorCore。
-   好用，约等于不能用C++。


## TVM/XLA这些AI编译器怎么样 {#tvm-xla这些ai编译器怎么样}

对于计算机科学来说，如果新问题出现，大家就会想加入一个新的抽象。算子在增多，计算芯片在增多。那么我需要一个自动生成内核的工具，不想手写kernel，也就是一个AI编译器。

TVM算是早期AI编译器中最成功的一个，其发起者陈天奇和Luis Ceze都是编译器背景。

但是：

-   TVM始终无法提供最佳性能，距离nvidia实现总是相差20%以上。
-   供应商对代码进行分叉，互相抱怨影响了开发进度
-   编译速度太慢

TVM的商业化公司OctoAI被Nvidia收购之后，大量管理层离职，TVM的未来也随之变得充满不确定性。

XLA是Google的尝试，基本解决了[集群训练](https://jax-ml.github.io/scaling-book/)的问题。但是XLA是私有的。

对应的OpenXLA是公开的，但是对于工程师来说，TPU总是第一优先级，从来不会优先考虑其他芯片。

同时因为迭代不够快，XLA也被Google自己抛弃，如今许多算子是在[Pallas](https://docs.jax.dev/en/latest/pallas/index.html)中实现的。

所以OpenCL的教训依旧存在：

-   需要参考实现，有进步。
-   强有力的领导，不算是。TVM碎片化严重。OpenXLA没有形成社区。
-   性能要好，都没有做到beat cuDNN。
-   还算好用，但是TVM编译时间（autotuning）太长了。


## Triton怎么样 {#triton怎么样}

Triton是一个DSL(Domain-Specific Language)，长得像python，但是不是python。

Triton提供了一种块级别的描述算子的方法，由编译器生成最优实现。并没有暴露所有的硬件能力。

结论来说，够易用，但是不够快。始终和最优实现（不管是Nvidia的库，还是手写的PTX算子）保持20%以上的差距。

除了Nvidia之外，其他芯片的支持也不够好。


## MLIR怎么样 {#mlir怎么样}

如上面所说，计算机这个圈子只要看到了新问题，第一反应就是加一层抽象。Chris和Jeff在2018年发现，好像有这么多AI编译器了（Glow/ONNX/TFLite/XLA/TVM）。

他们决定做一个编译器的框架，把编译器中用到的“编译器技术”统一起来：静态单赋值（SSA），代数化简，多面体分析。

用Chris的说法，MLIR自从先给LLVM基金会之后，得以腾飞，成为了众多项目的基础。

但是没有打破CUDA的垄断，因为：

-   参考实现似乎是有的，但是只是不同模块的拼装。
-   领导组织并没有维持稳定。
-   性能依旧有差距。
-   学习曲线陡峭，开发门槛搞。
-   也是走向了碎片化。


## 为什么硬件公司无法建立AI软件 {#为什么硬件公司无法建立ai软件}

在硬件公司的财务统计里面，芯片永远是产品，软件永远是间接费用。

AI软件不是和Nvidia的CUDA竞争，而是和整个AI生态系统做竞争，因为所有人都在优先投入在Nvidia的硬件上。


## 所以为什么Modular可以做到 {#所以为什么modular可以做到}

我觉得到这里就有点夹带私货了。

我觉得最核心的还是，Chris的Mojo真的可以做到最佳性能：[Achieving State-of-the-Art Performance on AMD MI355 — in Just 14 Days](https://www.modular.com/blog/achieving-state-of-the-art-performance-on-amd-mi355----in-just-14-days)。

归根到底是一个技术够强，可以平推的故事。
