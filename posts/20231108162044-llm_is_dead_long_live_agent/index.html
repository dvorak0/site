<!doctype html><html lang=zh-cn><head><meta charset=UTF-8><meta name=description content="self-reflections of Zhenfei Yang"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>LLM is dead, long live agent</title>
<link rel=stylesheet href=../../css/style.css></head><body><header><script defer>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?41e2e31fcfdb815d942715d44866f9fc",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><script defer src="https://www.googletagmanager.com/gtag/js?id=G-JHXS14NDHV"></script><script defer>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JHXS14NDHV")</script>========================<br>== <a href=https://www.yangzhenfei.com>Seeking Complexity</a> ==<br>========================<div style=float:right></div><br><p><nav><a href=../../><b>Start</b></a>.
<a href=../../posts/><b>Posts</b></a>.
<a href=../../tags/><b>Tags</b></a>.
<a href=../../about/><b>About</b></a>.</nav></p></header><main><article><h1>LLM is dead, long live agent</h1><b><time>2023-11-08 16:20:00</time></b>
<a href=../../tags/public>PUBLIC</a><div><figure><img src=../../ox-hugo/2023-11-08_16-22-44_screenshot_hub7097e7d50373e2bed1e8fe0bca8498d_594068_1280x0_resize_q75_h2_box_3.webp height=847 width=1280></figure><p>是这样吗？</p><p>自Tesla之后，又多了一个需要逐帧学习的发布会。</p><p>那么OpenAI在说什么。</p><p>确实需要把重点从world model移开，关注如何更好地使用这个先验模型。</p><p>就好像即使人出生的时候已经如此聪明了，还是要经历长时间地教育去获得更多慢思考的品质：耐心、共情、自省等等。</p><p>LLM给我们的，只有next ONE token generation接口，那么只是朴素地使用这个接口，能写出来最好的东西吗，现在看起来是未必的。</p><p>举一个最简单的例子：RAG(Retrieval Augmented Generation)是不是一个solved problem？不是，否则就不会有那么多的RAG方法。</p><figure><img src=../../ox-hugo/2023-11-08_16-34-10_screenshot_hu59238733bbf6eb0f9a26da02bc1ab8dd_903213_1280x0_resize_q75_h2_box_3.webp height=507 width=1280></figure><p>再结合yann lecun的挑战，我们确实能在这个空间里找到那个token sequences吗，尤其是在面对复杂任务的时候。</p><figure><img src=../../ox-hugo/2023-11-09_13-48-30_screenshot_hu67062663857f8740c2375e0ab7cf1757_547284_1280x0_resize_q75_h2_box_3.webp height=711 width=1280></figure><p>新的机会，但是也需要一些新的突破？</p></div></article></main><script src=https://utteranc.es/client.js repo=dvorak0/site issue-term=pathname theme=github-light crossorigin=anonymous async></script><script async src=https://www.yangzhenfei.com/js/mathjax-config.js></script><script type=text/javascript async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-chtml.min.js></script><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=../../posts/20231108162044-llm_is_dead_long_live_agent/>LLM is dead, long live agent</a></li><li><a href=../../posts/20231023124926-%E7%94%A8%E6%89%93%E5%8D%A1%E5%B8%AE%E5%8A%A9%E4%B9%A0%E6%83%AF%E5%85%BB%E6%88%90/>用打卡帮助习惯养成-Streaks</a></li><li><a href=../../posts/20230730115117-portfolio/>portfolio</a></li><li><a href=../../posts/20230609103642-first_principle/>how first principles thinking fails</a></li><li><a href=../../posts/20230531141820-system_1_and_system_2/>system_1_and_system_2</a></li></ul></div></div></aside><footer><p>&copy; 2023 <a href=https://www.yangzhenfei.com><b>Seeking Complexity</b></a>.</p></footer></body></html>