<!doctype html><html lang=zh-cn><head><meta charset=UTF-8><meta name=description content="self-reflections of Zhenfei Yang"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>双目相机的观测距离是怎么算出来的：Probabilistic Programming</title><link rel=stylesheet href=../../css/style.css></head><body><header><script defer src=https://worker.yangzhenfei.com/a.js></script><script defer>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JHXS14NDHV")</script><script defer src=https://cloud.umami.is/script.js data-website-id=045b5d99-a38d-410c-984f-5ac9e7cc7b76></script>========================<br>== <a href=https://www.yangzhenfei.com/>Seeking Complexity</a> ==<br>========================<div style=float:right></div><br><p><nav><a href=../../><b>Start</b></a>.
<a href=../../posts/><b>Posts</b></a>.
<a href=../../tags/><b>Tags</b></a>.
<a href=../../about/><b>About</b></a>.</nav></p></header><main><article><h1>双目相机的观测距离是怎么算出来的：Probabilistic Programming</h1><b><time>2022-05-20 11:19:00</time></b>
<a href=../../tags/public>PUBLIC</a><div><p>我们都知道贝叶斯定理，也知道围绕这个定理的一些反直觉的故事：比如在一定的假设下，去医院即使检测出疾病，也是不得病的概率更大。</p><p>但是我们没有一个很好用的工具，回答一系列这样的问题：在设定一些假设并经历一系列事件后，哪个结果出现的概率更高？</p><p>从计算机的角度看，我们缺少一个可以操作概率分布的工具库。</p><p>当然，这样一个轮子已经被发明了。</p><p>介绍一个我觉得很实用，但是并不常听说的编程工具：Probabilistic Programming Language</p><h2 id=贝叶斯定理101-癌症测试>贝叶斯定理101：癌症测试</h2><p>我们有以下假设：</p><ul><li>P(Cancer) = 1%
人群中有1%的人有癌症</li><li>P(Positive | Cancer) = 80%
如果有癌症，我们现在的检测手段可以以80%的概率检测出来</li><li>P(Positive | No Cancer) = 9.6%
如果没有癌症，也有9.6%的概率误报</li><li>我们想知道 P(Cancer | Positive)，也就是在检测结果为阳性的情况下，患癌的概率是多少。</li></ul><p>先说答案：即使检测出癌症，真实得癌症的概率是7.76%</p><p>贝叶斯定理：
\[
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
\]
我们有</p><p>\[
P(\text{Cancer} \mid \text{Positive}) = \frac{P(\text{Positive} \mid \text{Cancer}) \cdot P(\text{Cancer})}{P(\text{Positive})}
\]</p><p>接下来的变换用到了全概率公式。</p><p>\[
P(\text{Positive}) = P(\text{Positive} \mid \text{Cancer}) \cdot P(\text{Cancer}) + P(\text{Positive} \mid \text{No Cancer}) \cdot P(\text{No Cancer})
\]</p><p>\[
= (0.80 \times 0.01) + (0.096 \times 0.99)
\]</p><p>\[
= 0.008 + 0.09504 = 0.10304
\]</p><p>应用贝叶斯定理</p><p>\[
P(\text{Cancer} \mid \text{Positive}) = \frac{0.80 \times 0.01}{0.10304} = \frac{0.008}{0.10304} \approx 0.0776
\]</p><p>结论</p><p>\[
P(\text{Cancer} \mid \text{Positive}) \approx 7.76\%
\]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax.numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpyro
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpyro.distributions <span style=color:#66d9ef>as</span> dist
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> numpyro.infer <span style=color:#f92672>import</span> MCMC, DiscreteHMCGibbs, NUTS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mammogram_model</span>():
</span></span><span style=display:flex><span>    has_cancer <span style=color:#f92672>=</span> numpyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;has_cancer&#34;</span>,dist<span style=color:#f92672>.</span>Bernoulli(<span style=color:#ae81ff>0.01</span>))
</span></span><span style=display:flex><span>    prob_positive <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>where(has_cancer, <span style=color:#ae81ff>0.80</span>, <span style=color:#ae81ff>0.096</span>)
</span></span><span style=display:flex><span>    numpyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;test_result&#34;</span>, dist<span style=color:#f92672>.</span>Bernoulli(prob_positive), obs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Use DiscreteHMCGibbs with NUTS</span>
</span></span><span style=display:flex><span>kernel <span style=color:#f92672>=</span> DiscreteHMCGibbs(NUTS(mammogram_model))
</span></span><span style=display:flex><span>mcmc <span style=color:#f92672>=</span> MCMC(kernel, num_warmup<span style=color:#f92672>=</span><span style=color:#ae81ff>5000</span>, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>20000</span>)
</span></span><span style=display:flex><span>mcmc<span style=color:#f92672>.</span>run(random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>samples <span style=color:#f92672>=</span> mcmc<span style=color:#f92672>.</span>get_samples()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>posterior <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>mean(samples[<span style=color:#e6db74>&#34;has_cancer&#34;</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;P(Cancer | Positive) ≈ </span><span style=color:#e6db74>{</span>posterior<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> (</span><span style=color:#e6db74>{</span>posterior <span style=color:#f92672>*</span> <span style=color:#ae81ff>100</span><span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%)&#34;</span>)
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code class=language-nil data-lang=nil>sample: 100%|██████████| 25000/25000 [00:16&lt;00:00, 1513.40it/s, 1 steps of size 3.40e+38. acc. prob=1.00]
P(Cancer | Positive) ≈ 0.0786 (7.86%)
</code></pre><p>结果对了，但是不是准确的，因为整个方法是基于采样的。</p><h2 id=真实的例子-如何分析一个双目相机的测距精度>真实的例子：如何分析一个双目相机的测距精度</h2><p>在机器人中，经常使用双目相机来做双目观测。</p><p>一个常见的问题是：拿到一个相机，我们该如何分析它的观测精度？</p><p>首先我们需要了解双目测距的原理：</p><figure><img src=../../ox-hugo/2025-06-20_16-15-52_screenshot_hu_d04005c2ae1290da.webp height=764 width=1280></figure><p>用图中O_1, O_2, Cam_2的三角形和他的相似三角形，\(\frac{\text{disparity}}{\text{focal}} = \frac{\text{baseline}}{\text{depth}}\)，
也就是
\(\text{depth} = \frac{\text{baseline} \times \text{focal} }{\text{disparity}}\)。</p><p>所以，我们有几个观察：</p><ul><li>最重要的，距离和视差是倒数关系，时差越小，距离越远。而且不是线性的。</li><li>baseline 越大，测量的最大距离越远”</li><li>focal越大看得越远，对应FOV越小</li></ul><p>所以，为了方便，我们将视差为1像素时对应的深度称为“观测距离”。</p><p>比如一个相机是VGA分辨率640x480，FOV是90度，这两个参数粗略可以算出Focal Length是320。</p><p>baseline在5cm的情况下，观测距离是\(\frac{0.05m \times * 320px}{1px}=16m\)</p><p>然后，我的观测误差是多少呢。</p><p>我们还缺一个条件：disparity（视差）是如何定义的。</p><p>由于 disparity 是通过算法估算得出的，无法直接精确定义，只能通过实验拟合。</p><p>比如，我们先认为，他是一个均值是0，方差是0.3像素的高斯分布。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax.numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax <span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpyro
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpyro.distributions <span style=color:#66d9ef>as</span> dist
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> numpyro.infer <span style=color:#f92672>import</span> MCMC, NUTS, Predictive
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Known constants</span>
</span></span><span style=display:flex><span>baseline <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.05</span>     <span style=color:#75715e># meters</span>
</span></span><span style=display:flex><span>focal_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>320.0</span>  <span style=color:#75715e># pixels</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>depth_model</span>():
</span></span><span style=display:flex><span>    disparity <span style=color:#f92672>=</span> numpyro<span style=color:#f92672>.</span>sample(<span style=color:#e6db74>&#34;disparity&#34;</span>, dist<span style=color:#f92672>.</span>Normal(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.3</span>))
</span></span><span style=display:flex><span>    depth <span style=color:#f92672>=</span> (baseline <span style=color:#f92672>*</span> focal_length) <span style=color:#f92672>/</span> disparity
</span></span><span style=display:flex><span>    numpyro<span style=color:#f92672>.</span>deterministic(<span style=color:#e6db74>&#34;depth&#34;</span>, depth)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run inference with prior predictive sampling</span>
</span></span><span style=display:flex><span>rng_key <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>predictive <span style=color:#f92672>=</span> Predictive(depth_model, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>)
</span></span><span style=display:flex><span>depth_samples <span style=color:#f92672>=</span> predictive(rng_key)[<span style=color:#e6db74>&#39;depth&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Compute the mean</span>
</span></span><span style=display:flex><span>mean_depth <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>mean(depth_samples)
</span></span><span style=display:flex><span>std_depth <span style=color:#f92672>=</span> float(jnp<span style=color:#f92672>.</span>std(depth_samples))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Plot histogram</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>hist(depth_samples, density<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;skyblue&#39;</span>, bins<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>axvline(mean_depth, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, linestyle<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dashed&#39;</span>, linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Mean = </span><span style=color:#e6db74>{</span>mean_depth<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, Std = </span><span style=color:#e6db74>{</span>std_depth<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add labels and title</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Depth (Z)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Density&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Depth Distribution (from Gaussian disparity)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlim(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><figure><img src=../../ox-hugo/2025-06-20_16-39-37_screenshot_hu_de885574568201e9.webp height=997 width=1280></figure><p>有趣的结果：</p><ul><li>不是一个高斯分布，这是合理的。因为倒数不是一个线性运算，不会保持高斯分布。</li><li>均值不是在16m处。</li><li>得到的depth的标准差相当大，有16m<ul><li>这里插一句，如果不用采样的方法，我们通常会把depth_model线性展开，得到一个线性模型，然后我们就认为不管怎么传递，都能保留高斯分布。用这种方法算出来的结果是\(\mathcal{N}(16, 15.18^2)\)。</li></ul></li></ul><h2 id=一个小工具>一个小工具</h2><div id=depth-widget><style>#depth-widget{font-family:sans-serif;border:1px solid #ccc;border-radius:8px;padding:1em;max-width:400px;background:#f9f9f9;margin:1em auto}#depth-widget label{display:block;margin-top:.5em}#depth-widget input,#depth-widget select,#depth-widget button{width:100%;padding:.5em;margin-top:.2em;box-sizing:border-box}#depth-widget button{margin-top:1em;background:#007acc;color:#fff;border:none;cursor:pointer;border-radius:4px}#depth-widget button:hover{background:#005fa3}#depth-widget #result{margin-top:1em;font-weight:700}</style><p><label for=mode>Focal Length Input Mode:</label>
<select id=mode onchange=toggleFocalInput()><option value=manual>Manual Focal Length</option><option value=fov>Compute from FOV</option></select></p><div id=manualInput><label for=focalLength>Focal Length (px):</label>
<input type=number id=focalLength value=700></div><div id=fovInput style=display:none><label for=fov>Horizontal FOV (degrees):</label>
<input type=number id=fov value=90>
<label for=imageWidth>Image Width (px):</label>
<input type=number id=imageWidth value=640></div><p><label for=baseline>Baseline (m):</label>
<input type=number id=baseline step=0.01 value=0.1></p><p><label for=disparity>Disparity (px):</label>
<input type=number id=disparity step=0.1 value=1></p><p><button onclick=calculateDepth()>Calculate Depth</button></p><p id=result>Depth: -- m</p></div><script>function toggleFocalInput(){const e=document.getElementById("mode").value;document.getElementById("manualInput").style.display=e==="manual"?"block":"none",document.getElementById("fovInput").style.display=e==="fov"?"block":"none"}function calculateDepth(){const n=document.getElementById("mode").value;let e;if(n==="manual")e=parseFloat(document.getElementById("focalLength").value);else{const t=parseFloat(document.getElementById("fov").value)*Math.PI/180,n=parseFloat(document.getElementById("imageWidth").value);e=n/(2*Math.tan(t/2))}const s=parseFloat(document.getElementById("baseline").value),t=parseFloat(document.getElementById("disparity").value);if(t<=0){document.getElementById("result").textContent="Disparity must be greater than 0.";return}const o=e*s/t;document.getElementById("result").textContent=`Depth: ${o.toFixed(3)} m`}</script><h2 id=告别高斯分布>告别高斯分布</h2><p>正如用<a href=../../posts/20230918200243-mbti/>MBTI</a>来定义一个人是片面的，仅用均值和方差来描述一个分布也同样过于简化。</p><p>PPL给了我们一个方便的方法去查看不同类似的分布。</p><p>比如，有一些我很喜欢的结果：</p><ul><li><p>在visual localization中，用远处的点好不好呢？</p><p>结果是，远处的点会对旋转更有帮助，但是对位移没有帮助。</p><p>这也解释了我之前在做双目自标定时候的一个发现：无穷远处的点对于旋转标定的精度至关重要。</p></li><li><p>越来越多的数据对结果的提高是什么关系呢？比如SLAM中用的特征点数量。</p><p>我们直觉上知道，应该不是线性关系。</p><p>理论和PPL的实验都说明是一个开方关系，也就是说，数据量提升 100 倍，标准差大约能降低 10 倍。</p></li><li><p>一个slam的例子：<a href=https://taku-y.github.io/notebook/20170919/slam_advi.html>https://taku-y.github.io/notebook/20170919/slam_advi.html</a></p></li></ul><h2 id=不要幻想了-没法实时的>不要幻想了，没法实时的</h2><p>这么好的东西。如果万物都自带一个分布描述，这个世界该有多美好。</p><p>但是这套方法的主要基础，是类似Gibbs sampling的一套东西，和往地上投针估计PI没什么区别。</p><p>所以也谈不上效率和实时了，离线玩一玩还是可以的。</p></div></article></main><script src=https://utteranc.es/client.js repo=dvorak0/site issue-term=pathname theme=github-light crossorigin=anonymous async></script><script async src=https://www.yangzhenfei.com/js/mathjax-config.js></script><script type=text/javascript async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-chtml.min.js></script><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=../../posts/20250605170140-%E4%BA%BA%E7%B1%BBcot%E5%85%B8%E8%8C%83_%E5%BC%A0%E7%A5%A5%E9%9B%A8%E7%9A%84line_of_research/>人类CoT典范，张祥雨的Line of Research</a></li><li><a href=../../posts/20250516101712-building_music_theory_from_the_scratch/>Music Theory: Building from the Scratch</a></li><li><a href=../../posts/20250508121432-%E6%A8%A1%E4%BB%BF%E5%8F%AF%E4%BB%A5%E4%BA%A7%E7%94%9F%E6%99%BA%E8%83%BD%E5%90%97/>E2E Self-Driving: 模仿可以产生智能吗？</a></li><li><a href=../../posts/20250506141744-%E6%9D%8E%E5%85%89%E8%80%80%E8%A7%82%E5%A4%A9%E4%B8%8B/>《李光耀观天下》</a></li><li><a href=../../posts/20250506140144-%E8%A7%A3%E6%9E%84%E7%8E%B0%E4%BB%A3%E5%8C%96_%E6%B8%A9%E9%93%81%E5%86%9B%E6%BC%94%E8%AE%B2%E5%BD%95/>《解构现代化：温铁军演讲录》</a></li></ul></div></div></aside><footer><p>&copy; 2025 <a href=https://www.yangzhenfei.com/><b>Seeking Complexity</b></a>.
<a href=https://github.com/dvorak0><b>Github</b></a>.
<a href=https://twitter.com/dvorak0><b>Twitter</b></a>.</p></footer></body></html>