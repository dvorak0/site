<!doctype html><html lang=zh-cn><head><meta charset=UTF-8><meta name=description content="self-reflections of Zhenfei Yang"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>重新认识Chris Lattner</title><link rel=stylesheet href=../../css/style.css></head><body><header><script defer src=https://worker.yangzhenfei.com/a.js></script><script defer>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JHXS14NDHV")</script><script defer src=https://cloud.umami.is/script.js data-website-id=045b5d99-a38d-410c-984f-5ac9e7cc7b76></script>========================<br>== <a href=https://www.yangzhenfei.com/>Seeking Complexity</a> ==<br>========================<div style=float:right></div><br><p><nav><a href=../../><b>Start</b></a>.
<a href=../../posts/><b>Posts</b></a>.
<a href=../../tags/><b>Tags</b></a>.
<a href=../../about/><b>About</b></a>.</nav></p></header><main><article><h1>重新认识Chris Lattner</h1><b><time>2025-10-27 16:12:00</time></b>
<a href=../../tags/public>PUBLIC</a><div><p>Chris是谁？</p><ul><li>在Apple发明LLVM/Clang/Swift。</li><li>在Tesla另外自动驾驶芯片。</li><li>在Google发起MLIR</li><li>RISC-V商业化公司SiFive的VP</li><li>Modulr的创始人，发明Mojo</li></ul><p>一系列光环的大佬。</p><p>记录这个文章的起点是，发现Chris在和geohot（<a href=../../posts/20230510165215-%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86geohot/>重新认识geohot</a>中的geohot）就什么是AI compiler的终极形态的讨论：</p><figure><img src=../../ox-hugo/2025-10-27_16-16-01_screenshot_hu_e5feea9c27bc4e0.webp height=885 width=1280></figure><p>可以说是正统派和野路子的正面交锋/交流。</p><p>Chris写了一个系列文章：<a href=https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai>Democratizing AI Compute</a>，讲述了自己为什么要创业做Mojo。</p><p>读完之后，感觉终于解决了我一个长久的困惑，TVM/XLA/Triton/cuTile&mldr;似乎都在解决一个问题，AI编译器，他们究竟是什么关系。</p><p>所幸大佬站在浪潮之巅，亲身经历这一切，并慷慨地写了一个像是自传一样的总结，让我可以管中窥豹。</p><p>记录一下。</p><h2 id=deepseek对ai的影响>DeepSeek对AI的影响</h2><p>DeepSeek通过透明地承认自己在压榨GPU上的热爱和成就，戳破了一层窗户纸：</p><p>AI模型的工程化比学术化相比，更加是这个时代的中心，而GPU的利用水平拉开了不同玩家的差距。</p><p>Chris过去25年最多的时间投入在了LLVM上，这是一个编译器的编译器，为C++/Rust/Swift/OpenCL的语言奠定了编译器基础。</p><p>其中OpenCL则是最早的大家对于通用AI编译器抽象的愿景的化身。</p><p>在Google，Chris帮助Google做出了TPU，可以说是帮Attention/Bert等创新铺平了计算道路。</p><p>但是当别人问他：为什么CUDA这种东西依旧存在。Chris自己也觉得需要展开说说：</p><ul><li>为什么CUDA如此成功</li><li>为什么OpenCL并没有解决这个问题，而后继者Triton，OneAPI也没有解决这个问题。</li></ul><h2 id=到底什么是cuda>到底什么是CUDA</h2><p>是语法（CUDA C++），还是库（cuDNN）或者是解决方案（Triton Serving/TensorRT/TensorRT-LLM）。</p><p>2001年之前，GPU只能运行固定的render pipeline（transform/lightning/raster），直到shader开始支持可编程的图形效果。</p><p>人们发现，这里存在通用计算的可能。</p><p>2006年CUDA发布，用一种C++语法扩展的方案支持了并行计算。</p><p>然后很快地，在这种语言之上的库蓬勃发展，线性代数cuBLAS，傅里叶变换cuFFT，以及对后面影响最大的神经网络cuDNN。直接支撑起了TensorFlow和PyTorch。</p><p>然后又有：</p><ul><li>Triton Serving：模型推理服务</li><li>TensorRT：端测的模型编译部署</li><li>TensorRT-LLM：Nvidia的类似vLLM的LLM推理服务</li></ul><h2 id=cuda怎么成功的>CUDA怎么成功的</h2><p>只能马后炮的总结一下了：跨代兼容，坚定的投入软件，和PyTorch/TensorFlow并肩作战，AI迎来了GenAI这样真正的突破。</p><h2 id=cuda足够好吗>CUDA足够好吗</h2><p>垄断是每个人（除了nvidia）都担心的。</p><p>CUDA不具有100%的硬件控制力，所以FlashAttention不得不使用PTX来写。相比CUDA来说，PTX就更加是一个黑匣子了。</p><p>所以<a href=https://en.wikipedia.org/wiki/Jim_Keller_(engineer)>Jim Keller</a>有一个有名的论述：<a href=https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too>CUDA是一片沼泽，而不是护城河</a>。</p><h2 id=opencl怎么样>OpenCL怎么样</h2><p>OpenCL肩负了大家的愿景，一个硬件无关的AI计算标准，但是失败了。特别是发起者Apple自己都放弃了OpenCL，推出了自己的Metal。</p><p>Chris从中吸取了教训：</p><ul><li>不能只提供规范，没有参考实现。</li><li>强有力的领导，不能随意分叉，碎片化。</li><li>性能要顶级。目前OpenCL竟然都无法利用TensorCore。</li><li>好用，约等于不能用C++。</li></ul><h2 id=tvm-xla这些ai编译器怎么样>TVM/XLA这些AI编译器怎么样</h2><p>对于计算机科学来说，如果新问题出现，大家就会想加入一个新的抽象。算子在增多，计算芯片在增多。那么我需要一个自动生成内核的工具，不想手写kernel，也就是一个AI编译器。</p><p>TVM算是早期AI编译器中最成功的一个，其发起者陈天奇和Luis Ceze都是编译器背景。</p><p>但是：</p><ul><li>TVM始终无法提供最佳性能，距离nvidia实现总是相差20%以上。</li><li>供应商对代码进行分叉，互相抱怨影响了开发进度</li><li>编译速度太慢</li></ul><p>TVM的商业化公司OctoAI被Nvidia收购之后，大量管理层离职，TVM的未来也随之变得充满不确定性。</p><p>XLA是Google的尝试，基本解决了<a href=https://jax-ml.github.io/scaling-book/>集群训练</a>的问题。但是XLA是私有的。</p><p>对应的OpenXLA是公开的，但是对于工程师来说，TPU总是第一优先级，从来不会优先考虑其他芯片。</p><p>同时因为迭代不够快，XLA也被Google自己抛弃，如今许多算子是在<a href=https://docs.jax.dev/en/latest/pallas/index.html>Pallas</a>中实现的。</p><p>所以OpenCL的教训依旧存在：</p><ul><li>需要参考实现，有进步。</li><li>强有力的领导，不算是。TVM碎片化严重。OpenXLA没有形成社区。</li><li>性能要好，都没有做到beat cuDNN。</li><li>还算好用，但是TVM编译时间（autotuning）太长了。</li></ul><h2 id=triton怎么样>Triton怎么样</h2><p>Triton是一个DSL(Domain-Specific Language)，长得像python，但是不是python。</p><p>Triton提供了一种块级别的描述算子的方法，由编译器生成最优实现。并没有暴露所有的硬件能力。</p><p>结论来说，够易用，但是不够快。始终和最优实现（不管是Nvidia的库，还是手写的PTX算子）保持20%以上的差距。</p><p>除了Nvidia之外，其他芯片的支持也不够好。</p><h2 id=mlir怎么样>MLIR怎么样</h2><p>如上面所说，计算机这个圈子只要看到了新问题，第一反应就是加一层抽象。Chris和Jeff在2018年发现，好像有这么多AI编译器了（Glow/ONNX/TFLite/XLA/TVM）。</p><p>他们决定做一个编译器的框架，把编译器中用到的“编译器技术”统一起来：静态单赋值（SSA），代数化简，多面体分析。</p><p>用Chris的说法，MLIR自从先给LLVM基金会之后，得以腾飞，成为了众多项目的基础。</p><p>但是没有打破CUDA的垄断，因为：</p><ul><li>参考实现似乎是有的，但是只是不同模块的拼装。</li><li>领导组织并没有维持稳定。</li><li>性能依旧有差距。</li><li>学习曲线陡峭，开发门槛搞。</li><li>也是走向了碎片化。</li></ul><h2 id=为什么硬件公司无法建立ai软件>为什么硬件公司无法建立AI软件</h2><p>在硬件公司的财务统计里面，芯片永远是产品，软件永远是间接费用。</p><p>AI软件不是和Nvidia的CUDA竞争，而是和整个AI生态系统做竞争，因为所有人都在优先投入在Nvidia的硬件上。</p><h2 id=所以为什么modular可以做到>所以为什么Modular可以做到</h2><p>我觉得到这里就有点夹带私货了。</p><p>我觉得最核心的还是，Chris的Mojo真的可以做到最佳性能：<a href=https://www.modular.com/blog/achieving-state-of-the-art-performance-on-amd-mi355----in-just-14-days>Achieving State-of-the-Art Performance on AMD MI355 — in Just 14 Days</a>。</p><p>归根到底是一个技术够强，可以平推的故事。</p></div></article></main><script src=https://utteranc.es/client.js repo=dvorak0/site issue-term=pathname theme=github-light crossorigin=anonymous async></script><script async src=https://www.yangzhenfei.com/js/mathjax-config.js></script><script type=text/javascript async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-chtml.min.js></script><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=../../posts/20251031162057-%E7%AE%80%E6%98%8E%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA_%E5%86%99%E5%9C%A8%E7%94%B5%E7%A3%81%E6%82%AC%E6%8C%82%E5%9B%BD%E4%BA%A7%E5%8C%96%E7%9A%842025%E5%B9%B4/>简明控制理论：写在电磁悬挂国产化的2025年</a></li><li><a href=../../posts/20251027161231-%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86chris_lattner/>重新认识Chris Lattner</a></li><li><a href=../../posts/20250903175454-%E9%B1%BC%E5%92%8C%E7%86%8A%E6%8E%8C%E6%88%91%E8%A6%81%E5%85%BC%E5%BE%97_ros2%E9%80%82%E5%90%88%E9%87%8F%E4%BA%A7%E5%90%97/>鱼和熊掌我要兼得：ROS2适合量产吗</a></li><li><a href=../../posts/20250718102851-oop_considered_harmful/>OOP considered harmful</a></li><li><a href=../../posts/20250605170140-%E4%BA%BA%E7%B1%BBcot%E5%85%B8%E8%8C%83_%E5%BC%A0%E7%A5%A5%E9%9B%A8%E7%9A%84line_of_research/>人类CoT典范，张祥雨的Line of Research</a></li></ul></div></div></aside><footer><p>&copy; 2025 <a href=https://www.yangzhenfei.com/><b>Seeking Complexity</b></a>.
<a href=https://github.com/dvorak0><b>Github</b></a>.
<a href=https://twitter.com/dvorak0><b>Twitter</b></a>.</p></footer></body></html>